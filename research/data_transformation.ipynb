{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/Documents - Macbook’s MacBook Pro/career/career_chief_rep'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure config.yaml which holds information about where our data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data transformation\n",
    "data_transformation:\n",
    "  # Directory where data transformation results and artifacts are stored\n",
    "  root_dir: artifacts/data_transformation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/gsearch_jobs.csv\n",
    "\n",
    "  # Path to data validation status\n",
    "  data_validation: artifacts/data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the data transformation process.\n",
    "    \n",
    "    This configuration class captures the necessary paths and directories \n",
    "    required for the transformation of data post-ingestion and pre-model training.\n",
    "    \n",
    "    Attributes:\n",
    "    - root_dir: Directory where data transformation results and artifacts are stored.\n",
    "    - data_source_file: Path to the file where the ingested data is stored that needs to be transformed.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing transformation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested data file for transformation\n",
    "    data_validation: Path # Path to the validated output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.career_chief.constants import *\n",
    "from src.career_chief.utils.common import read_yaml, create_directories\n",
    "from src.career_chief import logger\n",
    "from src.career_chief.entity.config_entity import (DataIngestionConfig, DataValidationConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data transformation configurations as a DataTransformationConfig object.\n",
    "\n",
    "        This method fetches settings related to data transformation, like directories and file paths,\n",
    "        and returns them as a DataTransformationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataTransformationConfig: Object containing data transformation configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_transformation' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            \n",
    "            # Ensure the root directory for data transformation exists\n",
    "            create_directories([config.root_dir])\n",
    "\n",
    "            # Construct and return the DataTransformationConfig object\n",
    "            return DataTransformationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                data_validation=Path(config.data_validation),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_transformation' attribute does not exist in the config file.\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/miniconda3/envs/career_chief_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 06:13:37,921: 58: datasets: INFO: config:  PyTorch version 2.1.2 available.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checklist\n",
    "\n",
    "1. Check data validation status\n",
    "2. Read data\n",
    "3. Remove noise specific to technical resumes such as code snippets and special characters, and emojis\n",
    "4. Normalize by standardizing technical terminologies and acronyms\n",
    "5. Remove stop words and irrelevant words such as \"@\" in twitter mentions, or urls, pronouns, prepositions and conjunctions, etc\n",
    "6. Lemmatization, here we want the same token for different word forms, e.g. wolves and wolf, or talks and talk\n",
    "7. Stemming, here we remove and replace suffixes to get the root form of the word\n",
    "8. Tokenize the cleaned and normalized text to prepare it for further NLP and ML processing. tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER \")\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm  # Optional, for progress bar support\n",
    "\n",
    "from src.career_chief import logger\n",
    "from src.career_chief.entity.config_entity import DataTransformationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    Processes technical resume data for NLP tasks, performing cleaning, normalization,\n",
    "    and preparation steps such as stop word removal, lemmatization, stemming, and tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.df = self._load_data()\n",
    "        self._download_nltk_resources()\n",
    "        self.stop_words = self._initialize_stop_words()\n",
    "        self._handle_missing_values()\n",
    "        self.nlp_pipeline = self._initialize_nlp_pipeline()\n",
    "\n",
    "    def _download_nltk_resources(self):\n",
    "        \"\"\"Download necessary NLTK resources.\"\"\"\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "        logger.info(\"NLTK resources downloaded successfully.\")\n",
    "\n",
    "    def _initialize_stop_words(self):\n",
    "        \"\"\"Initialize the list of stop words from NLTK.\"\"\"\n",
    "        logger.info(\"Stop words initialized.\")\n",
    "        return set(stopwords.words('english'))\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Read data from the configured source.\"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.config.data_source_file)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _handle_missing_values(self):\n",
    "        \"\"\"Remove records with missing values in critical columns only.\"\"\"\n",
    "        critical_columns = ['description']  # List critical columns here\n",
    "        initial_shape = self.df.shape\n",
    "        self.df.dropna(subset=critical_columns, inplace=True)\n",
    "        final_shape = self.df.shape\n",
    "        logger.info(f\"Missing values handled in critical columns. Rows before: {initial_shape[0]}, after: {final_shape[0]}.\")\n",
    "        print(self.df)\n",
    "\n",
    "\n",
    "    def remove_noise(self, df):\n",
    "        \"\"\"Remove noise such as code snippets, special characters, and emojis.\"\"\"\n",
    "        # Example regex to remove special characters and emojis\n",
    "        df['cleaned_text'] = df['description'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)))\n",
    "        logger.info(\"Noise removed from the text.\")\n",
    "        return df\n",
    "\n",
    "    def normalize_technical_terms(self, df):\n",
    "        \"\"\"Normalize technical terminologies and acronyms.\"\"\"\n",
    "        normalization_dict = {\n",
    "            'AI': 'Artificial Intelligence',\n",
    "            'ML': 'Machine Learning',\n",
    "            # Add more terms and their normalized forms here\n",
    "        }\n",
    "        def normalize_text(text):\n",
    "            for term, normal_form in normalization_dict.items():\n",
    "                text = re.sub(r'\\b{}\\b'.format(term), normal_form, text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(normalize_text)\n",
    "        logger.info(\"Technical terminologies and acronyms normalized.\")\n",
    "        return df\n",
    "\n",
    "    def remove_stop_words(self, df):\n",
    "        \"\"\"Remove stop words and irrelevant words.\"\"\"\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in str(x).split() if word.lower() not in self.stop_words]))\n",
    "        logger.info(\"Stop words removed.\")\n",
    "        return df\n",
    "    \n",
    "    def lemmatize_text(self, df):\n",
    "        \"\"\"Lemmatize the text to get base forms of words.\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in str(x).split()]))\n",
    "        logger.info(\"Text lemmatized.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def stem_text(self, df):\n",
    "        \"\"\"Stem the text to get root forms of words.\"\"\"\n",
    "        stemmer = PorterStemmer()\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "        logger.info(\"Text stemmed.\")\n",
    "        return df\n",
    "    \n",
    "    def tokenize_text(self, df):\n",
    "        \"\"\"Tokenize the cleaned and normalized text.\"\"\"\n",
    "        # The tokenizer expects a list of texts, hence using `.tolist()`\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "        df['tokens'] = df['cleaned_text'].apply(lambda x: tokenizer.tokenize(str(x)))\n",
    "        logger.info(\"Text tokenized.\")\n",
    "        return df\n",
    "\n",
    "    def _initialize_nlp_pipeline(self):\n",
    "        \"\"\"Initialize the NLP pipeline for Named Entity Recognition (NER).\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "        logger.info(\"NER pipeline initialized.\")\n",
    "        return pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    def apply_ner(self, df):\n",
    "        \"\"\"\n",
    "        Applies Named Entity Recognition (NER) to the 'cleaned_text' column in batches,\n",
    "        with improved error handling and efficiency.\n",
    "        \"\"\"\n",
    "        batch_size = 100  # Adjust batch size based on memory and performance\n",
    "        total_batches = (len(df) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
    "\n",
    "        # Initialize an empty list to store NER results\n",
    "        ner_results = []\n",
    "\n",
    "        # Process DataFrame in batches\n",
    "        for batch_num in tqdm(range(total_batches), desc=\"Applying NER\"):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "            batch_texts = df.iloc[start_idx:end_idx]['cleaned_text'].tolist()\n",
    "\n",
    "            try:\n",
    "                # Apply NER pipeline to the entire list of texts in the batch\n",
    "                batch_ner_results = [self.nlp_pipeline(text) for text in batch_texts]\n",
    "                ner_results.extend(batch_ner_results)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"NER application failed for batch {batch_num} (indices {start_idx}-{end_idx}): {e}\")\n",
    "                # Append None or a specific error indicator for each item in the failed batch\n",
    "                ner_results.extend([None] * (end_idx - start_idx))\n",
    "\n",
    "        # Assign the NER results back to the DataFrame\n",
    "        df['ner_results'] = ner_results\n",
    "        logger.info(\"Named Entity Recognition applied in batches.\")\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Applies all preprocessing steps to a given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            Dataset: A Dataset object containing the transformed data.\n",
    "        \"\"\"\n",
    "        df = self.remove_noise(df)\n",
    "        df = self.normalize_technical_terms(df)\n",
    "        df = self.remove_stop_words(df)\n",
    "        df = self.lemmatize_text(df)\n",
    "        df = self.stem_text(df)\n",
    "        df = self.tokenize_text(df)\n",
    "        df = self.apply_ner(df)\n",
    "        return Dataset.from_pandas(df)\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.1, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the DataFrame into training, testing, and validation sets.\n",
    "\n",
    "        Args:\n",
    "            test_size (float): Fraction of the dataset to be used as test set.\n",
    "            val_size (float): Fraction of the dataset to be used as validation set.\n",
    "            random_state (int): Seed for random splitting for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Three DataFrames for training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        train_val_df, test_df = train_test_split(self.df, test_size=test_size, random_state=random_state)\n",
    "        adjusted_val_size = val_size / (1 - test_size)\n",
    "        train_df, val_df = train_test_split(train_val_df, test_size=adjusted_val_size, random_state=random_state)\n",
    "        return train_df, test_df, val_df\n",
    "    \n",
    "\n",
    "    def transform_train_test_val_data(self):\n",
    "        \"\"\"\n",
    "        Applies all preprocessing steps to a given DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            Dataset: A Dataset object containing the transformed data.\n",
    "        \"\"\"\n",
    "        train_df, test_df, val_df = self.split_data()\n",
    "        train_ds = self.transform_data(train_df)\n",
    "        test_ds = self.transform_data(test_df)\n",
    "        val_ds = self.transform_data(val_df)\n",
    "        return train_ds, test_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 06:38:26,883: 41: career_chief_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-06 06:38:26,887: 41: career_chief_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2024-03-06 06:38:26,890: 41: career_chief_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-06 06:38:26,890: 64: career_chief_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2024-03-06 06:38:26,891: 64: career_chief_logger: INFO: common:  Created directory at: artifacts/data_transformation]\n",
      "[2024-03-06 06:38:30,052: 20: career_chief_logger: INFO: 3758990984:  NLTK resources downloaded successfully.]\n",
      "[2024-03-06 06:38:30,053: 24: career_chief_logger: INFO: 3758990984:  Stop words initialized.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 06:38:30,272: 41: career_chief_logger: INFO: 3758990984:  Missing values handled in critical columns. Rows before: 37962, after: 37962.]\n",
      "       Unnamed: 0  index                                              title  \\\n",
      "0               0      0                                       Data Analyst   \n",
      "1               1      1                                       Data Analyst   \n",
      "2               2      2                          Aeronautical Data Analyst   \n",
      "3               3      3   Data Analyst - Consumer Goods - Contract to Hire   \n",
      "4               4      4                Data Analyst | Workforce Management   \n",
      "...           ...    ...                                                ...   \n",
      "37957       37957    600                     Marketing Data & BI Analyst II   \n",
      "37958       37958    601                                  Lead-Data Analyst   \n",
      "37959       37959    602                                  Lead-Data Analyst   \n",
      "37960       37960    603                                  Lead-Data Analyst   \n",
      "37961       37961    604  Institutional Credit Management - Lending Data...   \n",
      "\n",
      "                     company_name            location                    via  \\\n",
      "0                            Meta           Anywhere            via LinkedIn   \n",
      "1                             ATC    United States              via LinkedIn   \n",
      "2      Garmin International, Inc.       Olathe, KS                via Indeed   \n",
      "3                          Upwork           Anywhere              via Upwork   \n",
      "4                    Krispy Kreme    United States              via LinkedIn   \n",
      "...                           ...                 ...                    ...   \n",
      "37957                EDWARD JONES       Houstonia, MO  via My ArkLaMiss Jobs   \n",
      "37958                EDWARD JONES      Marshfield, MO  via My ArkLaMiss Jobs   \n",
      "37959                EDWARD JONES      High Point, MO  via My ArkLaMiss Jobs   \n",
      "37960                EDWARD JONES         Calhoun, MO  via My ArkLaMiss Jobs   \n",
      "37961                        Citi       United States  via My ArkLaMiss Jobs   \n",
      "\n",
      "                                             description  \\\n",
      "0      In the intersection of compliance and analytic...   \n",
      "1      Job Title: Entry Level Business Analyst / Prod...   \n",
      "2      Overview:\\n\\nWe are seeking a full-time...\\nAe...   \n",
      "3      Enthusiastic Data Analyst for processing sales...   \n",
      "4      Overview of Position\\n\\nThis position will be ...   \n",
      "...                                                  ...   \n",
      "37957  At Edward Jones, we help clients achieve their...   \n",
      "37958  At Edward Jones, we help clients achieve their...   \n",
      "37959  At Edward Jones, we help clients achieve their...   \n",
      "37960  At Edward Jones, we help clients achieve their...   \n",
      "37961  The Institutional Credit Management (ICM) grou...   \n",
      "\n",
      "                                              extensions  \\\n",
      "0      ['15 hours ago', '101K–143K a year', 'Work fro...   \n",
      "1      ['12 hours ago', 'Full-time', 'Health insurance']   \n",
      "2                          ['18 hours ago', 'Full-time']   \n",
      "3      ['12 hours ago', '15–25 an hour', 'Work from h...   \n",
      "4       ['7 hours ago', '90K–110K a year', 'Contractor']   \n",
      "...                                                  ...   \n",
      "37957  ['23 hours ago', '76,798–130,764 a year', 'Ful...   \n",
      "37958  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
      "37959  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
      "37960  ['23 hours ago', '106,916–182,047 a year', 'Fu...   \n",
      "37961  ['24 hours ago', '105,850–158,780 a year', 'Fu...   \n",
      "\n",
      "                                                  job_id  \\\n",
      "0      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
      "1      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QiLCJodGlkb2...   \n",
      "2      eyJqb2JfdGl0bGUiOiJBZXJvbmF1dGljYWwgRGF0YSBBbm...   \n",
      "3      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgLSBDb25zdW...   \n",
      "4      eyJqb2JfdGl0bGUiOiJEYXRhIEFuYWx5c3QgfCBXb3JrZm...   \n",
      "...                                                  ...   \n",
      "37957  eyJqb2JfdGl0bGUiOiJNYXJrZXRpbmcgRGF0YSBcdTAwMj...   \n",
      "37958  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
      "37959  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
      "37960  eyJqb2JfdGl0bGUiOiJMZWFkLURhdGEgQW5hbHlzdCIsIm...   \n",
      "37961  eyJqb2JfdGl0bGUiOiJJbnN0aXR1dGlvbmFsIENyZWRpdC...   \n",
      "\n",
      "                                               thumbnail  ... commute_time  \\\n",
      "0      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
      "1      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
      "2                                                    NaN  ...          NaN   \n",
      "3                                                    NaN  ...          NaN   \n",
      "4      https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
      "...                                                  ...  ...          ...   \n",
      "37957                                                NaN  ...          NaN   \n",
      "37958                                                NaN  ...          NaN   \n",
      "37959                                                NaN  ...          NaN   \n",
      "37960                                                NaN  ...          NaN   \n",
      "37961  https://encrypted-tbn0.gstatic.com/images?q=tb...  ...          NaN   \n",
      "\n",
      "          salary_pay salary_rate salary_avg salary_min salary_max  \\\n",
      "0          101K–143K      a year   122000.0   101000.0   143000.0   \n",
      "1                NaN         NaN        NaN        NaN        NaN   \n",
      "2                NaN         NaN        NaN        NaN        NaN   \n",
      "3              15–25     an hour       20.0       15.0       25.0   \n",
      "4           90K–110K      a year   100000.0    90000.0   110000.0   \n",
      "...              ...         ...        ...        ...        ...   \n",
      "37957   76798–130764      a year   103781.0    76798.0   130764.0   \n",
      "37958  106916–182047      a year   144481.5   106916.0   182047.0   \n",
      "37959  106916–182047      a year   144481.5   106916.0   182047.0   \n",
      "37960  106916–182047      a year   144481.5   106916.0   182047.0   \n",
      "37961  105850–158780      a year   132315.0   105850.0   158780.0   \n",
      "\n",
      "      salary_hourly  salary_yearly salary_standardized  \\\n",
      "0               NaN       122000.0            122000.0   \n",
      "1               NaN            NaN                 NaN   \n",
      "2               NaN            NaN                 NaN   \n",
      "3              20.0            NaN             41600.0   \n",
      "4               NaN       100000.0            100000.0   \n",
      "...             ...            ...                 ...   \n",
      "37957           NaN       103781.0            103781.0   \n",
      "37958           NaN       144481.5            144481.5   \n",
      "37959           NaN       144481.5            144481.5   \n",
      "37960           NaN       144481.5            144481.5   \n",
      "37961           NaN       132315.0            132315.0   \n",
      "\n",
      "                                      description_tokens  \n",
      "0                      ['sql', 'r', 'python', 'tableau']  \n",
      "1                                                     []  \n",
      "2                                                ['sql']  \n",
      "3                    ['excel', 'power_bi', 'powerpoint']  \n",
      "4             ['word', 'excel', 'outlook', 'powerpoint']  \n",
      "...                                                  ...  \n",
      "37957  ['python', 'snowflake', 'tableau', 'excel', 'p...  \n",
      "37958                                                 []  \n",
      "37959                                                 []  \n",
      "37960                                                 []  \n",
      "37961                              ['cognos', 'tableau']  \n",
      "\n",
      "[37962 rows x 27 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from src.career_chief.entity.config_entity import DataTransformationConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "class DataTransformationPipeline:\n",
    "    STAGE_NAME = \"Data Transformation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformationPipeline with necessary configurations\n",
    "        obtained from the ConfigurationManager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "        transformation_config = self.config_manager.get_data_transformation_config()\n",
    "        self.data_transformation = DataTransformation(transformation_config)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Executes the data transformation process.\n",
    "        \"\"\"\n",
    "        logger.info(f\"{self.STAGE_NAME}: Starting data transformation process.\")\n",
    "\n",
    "        # Load and preprocess the data\n",
    "        df = self.data_transformation._load_data()\n",
    "        df = self.data_transformation._handle_missing_values()\n",
    "        df = self.data_transformation.remove_noise(df)\n",
    "        df = self.data_transformation.normalize_technical_terms(df)\n",
    "        df = self.data_transformation.remove_stop_words(df)\n",
    "        df = self.data_transformation.lemmatize_text(df)\n",
    "        df = self.data_transformation.stem_text(df)\n",
    "        df = self.data_transformation.tokenize_text(df)\n",
    "        df = self.data_transformation.apply_ner(df)\n",
    "\n",
    "        # Optionally, split the data and transform into Dataset objects\n",
    "        train_df, test_df, val_df = self.data_transformation.split_data()\n",
    "        train_ds = Dataset.from_pandas(train_df)\n",
    "        test_ds = Dataset.from_pandas(test_df)\n",
    "        val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "        # logger.info(f\"{self.STAGE_NAME}: Data transformation process completed.\")\n",
    "\n",
    "        return train_ds, test_ds, val_ds\n",
    "\n",
    "    def transform_new_data(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Applies the data transformation process to a new dataset.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The file path to the new dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dataset: A Dataset object containing the transformed new data.\n",
    "        \"\"\"\n",
    "        # Adjust the configuration temporarily for the new dataset\n",
    "        original_file_path = self.data_transformation.config.data_source_file\n",
    "        self.data_transformation.config.data_source_file = file_path\n",
    "\n",
    "        logger.info(f\"{self.STAGE_NAME}: Transforming new dataset from {file_path}.\")\n",
    "        new_df = self.data_transformation._load_data()\n",
    "        new_df = self.data_transformation._handle_missing_values()\n",
    "        # Apply all transformation steps as in run_pipeline\n",
    "        new_df = self.data_transformation.remove_noise(new_df)\n",
    "        # Continue applying all other steps...\n",
    "        new_df = self.data_transformation.apply_ner(new_df)\n",
    "        transformed_new_ds = Dataset.from_pandas(new_df)\n",
    "\n",
    "        # Restore the original data source file path\n",
    "        self.data_transformation.config.data_source_file = original_file_path\n",
    "\n",
    "        return transformed_new_ds\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Initialize the pipeline\n",
    "    pipeline = DataTransformationPipeline()\n",
    "\n",
    "    # Execute the pipeline\n",
    "    train_ds, test_ds, val_ds = pipeline.run_pipeline()\n",
    "\n",
    "    # Here you can interact with the returned datasets\n",
    "    # For demonstration, we'll simply print the sizes of these datasets\n",
    "    print(f\"Training dataset size: {len(train_ds)}\")\n",
    "    print(f\"Testing dataset size: {len(test_ds)}\")\n",
    "    print(f\"Validation dataset size: {len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "career_chief_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
