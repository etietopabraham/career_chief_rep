{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/Documents - Macbookâ€™s MacBook Pro/career/career_chief_rep'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2308916172.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    spacy_ner:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Configuration for spaCy Named Entity Recognition (NER) model training\n",
    "spacy_ner:\n",
    "  root_dir: artifacts/model_training/spacy_ner  # Root directory for training artifacts\n",
    "  json_annotated_path: artifacts/model_training/spacy_ner/project-4-at-2024-04-01-07-30-2333a63c.json  # Path to annotated data\n",
    "  output_path: artifacts/model_training/spacy_ner/output/converted_data.spacy  # Output path for the converted spaCy data\n",
    "  train_data_path: artifacts/data_transformation/train_data.csv  # Path to unannotated training data\n",
    "  test_data_path: artifacts/data_transformation/test_data.csv  # Path to unannotated test data\n",
    "  val_data_path: artifacts/data_transformation/val_data.csv  # Path to unannotated validation data\n",
    "  spacy_train: artifacts/model_training/spacy_ner/output/train_data.spacy  # Processed spaCy training data\n",
    "  spacy_dev: artifacts/model_training/spacy_ner/output/dev_data.spacy  # Processed spaCy dev (validation) data\n",
    "  gpu_allocator: pytorch  # GPU allocator (use 'pytorch' for PyTorch)\n",
    "  components:  # NLP pipeline components\n",
    "    - name: \"ner\"\n",
    "      factory: \"ner\"\n",
    "  training:  # Training parameters\n",
    "    batch_size: 128\n",
    "    dropout: 0.5\n",
    "    optimizer:\n",
    "      learn_rate: 0.001\n",
    "    patience: 3\n",
    "    max_epochs: 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class SpacyNERConfig:\n",
    "    \"\"\"\n",
    "    Represents the configuration for spaCy Named Entity Recognition (NER) model training.\n",
    "\n",
    "    This class is intended to be populated with values from a YAML configuration file,\n",
    "    providing structured access to the configurations within the Python codebase.\n",
    "    \n",
    "    Attributes:\n",
    "        root_dir (Path): Directory for storing training artifacts and results.\n",
    "        json_annotated_path (Path): Path to the JSON file with annotations from Label Studio.\n",
    "        output_path (Path): Destination path for the converted spaCy data format.\n",
    "        train_data_path (Path): Path to the CSV file containing unannotated training data.\n",
    "        test_data_path (Path): Path to the CSV file containing unannotated test data.\n",
    "        val_data_path (Path): Path to the CSV file containing unannotated validation data.\n",
    "        spacy_train (Path): Path for the processed spaCy training data.\n",
    "        spacy_dev (Path): Path for the processed spaCy development (validation) data.\n",
    "        gpu_allocator (str): The GPU allocator for training, e.g., 'pytorch'.\n",
    "        components (List[Dict[str, Any]]): Configuration for the NER pipeline components.\n",
    "        training (Dict[str, Any]): Dictionary containing the training parameters.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    json_annotated_path: Path\n",
    "    output_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    val_data_path: Path\n",
    "    spacy_train: Path\n",
    "    spacy_dev: Path\n",
    "    gpu_allocator: str\n",
    "    components: List[Dict[str, Any]]\n",
    "    training: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from src.career_chief.constants import *\n",
    "from src.career_chief.utils.common import read_yaml, create_directories\n",
    "from src.career_chief import logger\n",
    "from src.career_chief.entity.config_entity import (DataIngestionConfig, DataValidationConfig, SpacyNERConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def get_spacy_ner_config(self) -> SpacyNERConfig:\n",
    "        \"\"\"\n",
    "        Fetches and constructs the spaCy NER training configuration.\n",
    "\n",
    "        Extracts settings related to spaCy NER model training from the loaded YAML\n",
    "        configurations and returns them encapsulated in a SpacyNERConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - SpacyNERConfig: Configuration object for spaCy NER model training.\n",
    "\n",
    "        Raises:\n",
    "        - KeyError: If any required configuration is missing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ner_config = self.config['spacy_ner']\n",
    "            \n",
    "            # Dynamically construct and return the SpacyNERConfig object\n",
    "            return SpacyNERConfig(\n",
    "                root_dir=Path(ner_config['root_dir']),\n",
    "                json_annotated_path=Path(ner_config['json_annotated_path']),\n",
    "                output_path=Path(ner_config['output_path']),\n",
    "                train_data_path=Path(ner_config['train_data_path']),\n",
    "                test_data_path=Path(ner_config['test_data_path']),\n",
    "                val_data_path=Path(ner_config['val_data_path']),\n",
    "                spacy_train=Path(ner_config['spacy_train']),\n",
    "                spacy_dev=Path(ner_config['spacy_dev']),\n",
    "                gpu_allocator=ner_config['gpu_allocator'],\n",
    "                components=ner_config['components'],\n",
    "                training=ner_config['training']\n",
    "            )\n",
    "\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"A required configuration is missing in the 'spacy_ner' section: {e}\")\n",
    "            raise KeyError(f\"Missing configuration in 'spacy_ner': {e}\") from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from src.career_chief import logger\n",
    "\n",
    "class SpacyCustomNERModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.nlp = spacy.blank(\"en\")  # Create a blank English model\n",
    "        if \"ner\" not in self.nlp.pipe_names:\n",
    "            self.ner = self.nlp.add_pipe(\"ner\")\n",
    "\n",
    "    # def process_annotations(self, annotated_data):\n",
    "    #     doc_bin = DocBin()\n",
    "    #     for item in annotated_data:\n",
    "    #         doc = self.nlp.make_doc(item[\"text\"])\n",
    "    #         ents = []\n",
    "    #         for start, end, label in item[\"entities\"]:\n",
    "    #             ents.append(doc.char_span(start, end, label=label, alignment_mode=\"strict\"))\n",
    "    #         doc.ents = ents\n",
    "    #         doc_bin.add(doc)\n",
    "    #     return doc_bin\n",
    "\n",
    "    def process_annotations(self, data, nlp):\n",
    "        \"\"\"\n",
    "        Processes annotations from a structured JSON format into spaCy's binary DocBin format.\n",
    "        \n",
    "        This function takes a list of annotated items, where each item includes the text and its annotations,\n",
    "        and processes these into a spaCy-compatible format for NER training. Each annotation is used to create\n",
    "        a `Span` object in spaCy, marking entity boundaries and their labels within the document text.\n",
    "        \n",
    "        Args:\n",
    "            data (list): A list of annotated data items. Each item is expected to have keys 'data' and 'annotations',\n",
    "                        where 'data' contains the text and 'annotations' is a list of annotation objects.\n",
    "            nlp (Language): An instance of a spaCy Language object for document processing.\n",
    "        \n",
    "        Returns:\n",
    "            DocBin: A DocBin object containing the processed documents with entities marked.\n",
    "        \"\"\"\n",
    "        doc_bin = DocBin()  # Initialize DocBin to store processed documents.\n",
    "\n",
    "        # Iterate over each annotated item in the input data.\n",
    "        for item in data:\n",
    "            # Extract text for the current item.\n",
    "            text = item[\"data\"][\"text\"]\n",
    "            # Tokenize the text using the provided spaCy Language object.\n",
    "            doc = nlp.make_doc(text)\n",
    "\n",
    "            # Prepare a list to store entity spans.\n",
    "            ents = []\n",
    "            # Set to track existing spans and avoid overlaps.\n",
    "            existing_spans = set()\n",
    "\n",
    "            # Iterate over each annotation in the current item.\n",
    "            for annot in item[\"annotations\"][0][\"result\"]:\n",
    "                # Extract start and end indices, and the entity label.\n",
    "                start = annot[\"value\"][\"start\"]\n",
    "                end = annot[\"value\"][\"end\"]\n",
    "                label = annot[\"value\"][\"labels\"][0]\n",
    "\n",
    "                # Ensure the new span does not overlap with existing ones.\n",
    "                if not any((start <= s < end) or (start < e <= end) for s, e in existing_spans):\n",
    "                    # Create a span from the annotation.\n",
    "                    span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "                    # If the span is valid (i.e., not None), add it to the list of entities.\n",
    "                    if span is not None:\n",
    "                        ents.append(span)\n",
    "                        # Record the span boundaries to check for future overlaps.\n",
    "                        existing_spans.add((start, end))\n",
    "\n",
    "            # Update the document with identified entities.\n",
    "            doc.ents = ents\n",
    "            # Add the document to DocBin.\n",
    "            doc_bin.add(doc)\n",
    "        \n",
    "        # Log completion of annotation processing.\n",
    "        logger.info(\"Annotations processed successfully.\")\n",
    "        return doc_bin\n",
    "\n",
    "\n",
    "    def split_and_save_data(self, docs, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Splits the list of spaCy Doc objects into training and development sets,\n",
    "        then saves them as .spacy files.\n",
    "\n",
    "        Args:\n",
    "            docs (list): The full list of spaCy Doc objects.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "        \"\"\"\n",
    "        # Split the docs into training and development sets\n",
    "        train_docs, dev_docs = train_test_split(docs, test_size=test_size, random_state=42)\n",
    "        logger.info(f\"Split data into {len(train_docs)} training and {len(dev_docs)} development examples.\")\n",
    "\n",
    "        # Save the train and dev DocBins\n",
    "        train_bin = DocBin(docs=train_docs, store_user_data=True)\n",
    "        dev_bin = DocBin(docs=dev_docs, store_user_data=True)\n",
    "        train_bin.to_disk(self.config.spacy_train)\n",
    "        dev_bin.to_disk(self.config.spacy_dev)\n",
    "        logger.info(\"Training and development data saved successfully.\")\n",
    "\n",
    "    def get_labels(self, docs):\n",
    "        \"\"\"\n",
    "        Extracts unique entity labels from the provided spaCy documents.\n",
    "\n",
    "        Args:\n",
    "            docs (list of spacy.Doc): List of spaCy Doc objects containing annotations.\n",
    "\n",
    "        Returns:\n",
    "            set: A set containing unique entity labels.\n",
    "        \"\"\"\n",
    "        labels = set()\n",
    "        for doc in docs:\n",
    "            for ent in doc.ents:\n",
    "                labels.add(ent.label_)\n",
    "        return labels\n",
    "    \n",
    "    def train_ner_model(self):\n",
    "        \"\"\"\n",
    "        Trains a custom NER model using the provided configuration.\n",
    "        \"\"\"\n",
    "        # Load or create a blank spaCy model\n",
    "        if self.config.gpu_allocator:\n",
    "            spacy.require_gpu()\n",
    "        nlp = spacy.blank(\"en\")\n",
    "\n",
    "        # Check if 'ner' is not in the pipeline, add it\n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.add_pipe('ner', last=True)\n",
    "        else:\n",
    "            ner = nlp.get_pipe(\"ner\")\n",
    "        \n",
    "        # Load training and development data\n",
    "        train_docs = list(DocBin().from_disk(self.config.spacy_train).get_docs(nlp.vocab))\n",
    "        dev_docs = list(DocBin().from_disk(self.config.spacy_dev).get_docs(nlp.vocab))\n",
    "\n",
    "        # Add labels to NER from training and development docs\n",
    "        for label in self.get_labels(train_docs + dev_docs):\n",
    "            ner.add_label(label)\n",
    "        \n",
    "        # Disable other pipelines during training\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "        with nlp.disable_pipes(*other_pipes):\n",
    "            optimizer = nlp.begin_training()\n",
    "\n",
    "            for itn in range(self.config.training[\"max_epochs\"]):\n",
    "                random.shuffle(train_docs)\n",
    "                losses = {}\n",
    "\n",
    "                # Batch up the examples using spaCy's minibatch\n",
    "                batches = minibatch(train_docs, size=compounding(4., 32., 1.001))\n",
    "                for batch in batches:\n",
    "                    examples = [Example.from_dict(nlp.make_doc(doc.text), {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]}) for doc in batch]\n",
    "                    \n",
    "                    # Update the model\n",
    "                    nlp.update(examples, drop=0.5, losses=losses)\n",
    "                print(f\"Losses at iteration {itn}: {losses}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        model_output_dir = Path(self.config.root_dir) / \"trained_model\"\n",
    "        nlp.to_disk(model_output_dir)\n",
    "        logger.info(f\"Custom NER model trained and saved successfully at {model_output_dir}.\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        # Load annotations from the JSON file\n",
    "        with open(self.config.json_annotated_path, 'r', encoding='utf-8') as f:\n",
    "            annotated_data = json.load(f)\n",
    "        \n",
    "        # Initialize a blank English spaCy model\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        logger.info(\"Blank spaCy model initialized.\")\n",
    "\n",
    "        # Process annotations using the loaded data and the initialized nlp object\n",
    "        doc_bin = self.process_annotations(annotated_data, nlp)\n",
    "        \n",
    "        # Assuming split_and_save_data expects a list of docs, not a DocBin\n",
    "        # Convert DocBin to list of docs for splitting and saving\n",
    "        docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "        # Split the processed annotations into training and development data and save them\n",
    "        self.split_and_save_data(docs)\n",
    "        \n",
    "        # Train the custom NER model\n",
    "        self.train_ner_model()\n",
    "\n",
    "        # Optionally, save the trained model to disk\n",
    "        self.nlp.to_disk(Path(self.config.root_dir) / \"trained_model\")\n",
    "\n",
    "        logger.info(\"SpacyCustomNERModel pipeline completed successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import spacy\n",
    "# from spacy.tokens import DocBin\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from spacy.training import Example\n",
    "# from spacy.util import minibatch, compounding\n",
    "# import random\n",
    "\n",
    "# from src.career_chief import logger\n",
    "# from src.career_chief.config.configuration import ConfigurationManager\n",
    "\n",
    "# class SpacyCustomNERModel:\n",
    "#     \"\"\"\n",
    "#     A class for handling the preparation, training, and evaluation of a custom spaCy Named Entity Recognition (NER) model.\n",
    "    \n",
    "#     Attributes:\n",
    "#         config (ConfigurationManager): An instance of ConfigurationManager to access configuration details.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, config: SpacyNERConfig):\n",
    "#         \"\"\"\n",
    "#         Initializes SpacyCustomNERModel with the specific configurations for spaCy NER model training.\n",
    "        \n",
    "#         Args:\n",
    "#             config (SpacyNERConfig): Configuration object containing all necessary settings for the NER model.\n",
    "#         \"\"\"\n",
    "#         self.config = config\n",
    "#         logger.info(\"SpacyCustomNERModel initialized with provided SpacyNERConfig.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     def process_annotations(self, data, nlp):\n",
    "#         \"\"\"\n",
    "#         Processes annotations from a structured JSON format into spaCy's binary DocBin format.\n",
    "        \n",
    "#         Args:\n",
    "#             data (list): A list of annotated data items.\n",
    "#             nlp (Language): An instance of a spaCy Language object for document processing.\n",
    "#         \"\"\"\n",
    "#         doc_bin = DocBin()  # Initialize DocBin to store docs\n",
    "\n",
    "#         for item in data:\n",
    "#             text = item[\"data\"][\"text\"]\n",
    "#             annotations = item[\"annotations\"][0][\"result\"]\n",
    "#             doc = nlp.make_doc(text)  # Tokenize the text\n",
    "\n",
    "#             ents = []\n",
    "#             existing_spans = set()  # Track spans to avoid overlaps\n",
    "#             for annot in annotations:\n",
    "#                 start = annot[\"value\"][\"start\"]\n",
    "#                 end = annot[\"value\"][\"end\"]\n",
    "#                 label = annot[\"value\"][\"labels\"][0]\n",
    "\n",
    "#                 # Check for overlap\n",
    "#                 if not any((start <= s < end) or (start < e <= end) for s, e in existing_spans):\n",
    "#                     span = doc.char_span(start, end, label=label, alignment_mode=\"strict\")\n",
    "#                     if span is not None:\n",
    "#                         ents.append(span)\n",
    "#                         existing_spans.add((start, end))\n",
    "\n",
    "#             # Update the document with the entities\n",
    "#             doc.ents = ents\n",
    "#             doc_bin.add(doc)\n",
    "        \n",
    "#         logger.info(\"Annotations processed successfully.\")\n",
    "#         return doc_bin\n",
    "\n",
    "#     def save_spacy_annotations(self, doc_bin, output_path):\n",
    "#         \"\"\"\n",
    "#         Saves the processed annotations to disk in spaCy's binary format.\n",
    "\n",
    "#         Args:\n",
    "#             doc_bin (DocBin): The DocBin object containing processed annotations.\n",
    "#             output_path (str): Path where the binary file will be saved.\n",
    "#         \"\"\"\n",
    "#         doc_bin.to_disk(output_path)\n",
    "#         print(f\"Processed data saved to: {output_path}\")\n",
    "#         logger.info(f\"Processed data saved to: {output_path}\")\n",
    "        \n",
    "#     def read_spacy_annotations(self, file_path):\n",
    "#         \"\"\"\n",
    "#         Reads and prints spaCy annotations from a .spacy file for verification.\n",
    "        \n",
    "#         Args:\n",
    "#             file_path (str): Path to the .spacy file containing annotated data.\n",
    "#         \"\"\"\n",
    "#         nlp = spacy.blank(\"en\")  # Load the blank English model\n",
    "#         doc_bin = DocBin().from_disk(file_path)  # Load the DocBin file\n",
    "#         docs = list(doc_bin.get_docs(nlp.vocab))  # Create a list of Doc objects\n",
    "        \n",
    "#         for doc in docs:\n",
    "#             print(f\"Text: {doc.text[:50]}...\")  # Print document text\n",
    "#             for ent in doc.ents:\n",
    "#                 print(f\" - Entity: {ent.text}, Label: {ent.label_}\")  # Print each entity and its label\n",
    "#             print(\"\\n---\\n\")\n",
    "\n",
    "#         logger.info(f\"Annotations from {file_path} read and printed for verification.\")\n",
    "#         return docs\n",
    "\n",
    "\n",
    "#     def split_and_save_data(self, annotated_data, test_size=0.2):\n",
    "#         \"\"\"\n",
    "#         Splits the annotated data into training and development sets, then saves them as .spacy files.\n",
    "\n",
    "#         Args:\n",
    "#             annotated_data (list): The full list of annotated examples.\n",
    "#             test_size (float): The proportion of the dataset to include in the test split.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Initialize blank English model\n",
    "#         nlp = spacy.blank(\"en\")\n",
    "\n",
    "#         # Load the processed and annotated data\n",
    "#         docs = self.read_spacy_annotations(self.config.output_path)\n",
    "\n",
    "#         # Split the annotated data into training and development sets\n",
    "#         train_docs, dev_docs = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "#         logger.info(f\"Split annotated data into {len(train_docs)} training and {len(dev_docs)} development examples.\")\n",
    "\n",
    "#         # Create DocBin for train and dev, and add respective docs\n",
    "#         train_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n",
    "#         for doc in train_docs:\n",
    "#             train_bin.add(doc)\n",
    "\n",
    "#         dev_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n",
    "#         for doc in dev_docs:\n",
    "#             dev_bin.add(doc)\n",
    "\n",
    "#         # Save the train and dev DocBins\n",
    "#         train_bin.to_disk(self.config.spacy_train)\n",
    "#         dev_bin.to_disk(self.config.spacy_dev)\n",
    "    \n",
    "#     def get_labels(self, data):\n",
    "#         \"\"\"\n",
    "#         Extracts unique labels from the annotated data.\n",
    "\n",
    "#         Args:\n",
    "#             data (list): A list of spacy.Doc objects from training and development data.\n",
    "\n",
    "#         Returns:\n",
    "#             set: A set of unique labels.\n",
    "#         \"\"\"\n",
    "#         labels = set()\n",
    "#         for doc in data:\n",
    "#             for ent in doc.ents:\n",
    "#                 labels.add(ent.label_)\n",
    "#         return labels\n",
    "\n",
    "    \n",
    "#     def train_ner_model(self):\n",
    "#         \"\"\"\n",
    "#         Trains a custom NER model using the split and saved training and development datasets.\n",
    "#         \"\"\"\n",
    "#         # Load the spaCy model\n",
    "#         if self.config.gpu_allocator:\n",
    "#             spacy.require_gpu()\n",
    "\n",
    "#         nlp = spacy.blank(\"en\")  # Create a blank English model\n",
    "        \n",
    "#         ## Check if the 'ner' component already exists in the pipeline\n",
    "#         # If not, add it\n",
    "#         if not nlp.has_pipe(\"ner\"):\n",
    "#             nlp.add_pipe(\"ner\")\n",
    "\n",
    "#         # Now, you can access the NER component and add labels, train, etc.\n",
    "#         ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "#         # Load training and development data\n",
    "#         train_bin = DocBin().from_disk(self.config.spacy_train)\n",
    "#         dev_bin = DocBin().from_disk(self.config.spacy_dev)\n",
    "\n",
    "#         train_data = list(train_bin.get_docs(nlp.vocab))\n",
    "#         dev_data = list(dev_bin.get_docs(nlp.vocab))\n",
    "        \n",
    "#         # Add the NER pipe to the pipeline if it doesn't exist\n",
    "#         # Create a new NER pipe if needed\n",
    "#         if \"ner\" not in nlp.pipe_names:\n",
    "#             ner = nlp.create_pipe(\"ner\")\n",
    "#             nlp.add_pipe(ner)\n",
    "#         else:\n",
    "#             ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "#         # Add labels to the NER pipe\n",
    "#         for label in self.get_labels(train_data + dev_data):  # Implement get_labels to extract unique labels\n",
    "#             ner.add_label(label)\n",
    "\n",
    "#         # Get names of other pipes to disable them during training\n",
    "#         other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "#         with nlp.disable_pipes(*other_pipes):  # Train only NER\n",
    "#             optimizer = nlp.begin_training()\n",
    "            \n",
    "#             for itn in range(self.config.training[\"max_epochs\"]):\n",
    "#                 losses = {}\n",
    "#                 batches = spacy.util.minibatch(train_data, size=self.config.training[\"batch_size\"])\n",
    "#                 for batch in batches:\n",
    "#                     texts, annotations = zip(*batch)\n",
    "#                     nlp.update(texts, annotations, sgd=optimizer, drop=self.config.training[\"dropout\"], losses=losses)\n",
    "#                 print(\"Losses\", losses)  # Optionally, implement more sophisticated logging\n",
    "\n",
    "#         # Save the trained model\n",
    "#         nlp.to_disk(Path(self.config.root_dir) / \"trained_model\")\n",
    "#         logger.info(\"Custom NER model trained and saved successfully.\")\n",
    "\n",
    "\n",
    "#     def run(self):\n",
    "#         \"\"\"\n",
    "#         Executes the full pipeline for spaCy NER model training.\n",
    "#         This includes processing annotations, splitting data, saving processed data,\n",
    "#         and training the custom NER model.\n",
    "#         \"\"\"\n",
    "#         logger.info(\"Starting the SpacyCustomNERModel pipeline.\")\n",
    "        \n",
    "#         # Load annotations from the annotated data file\n",
    "#         file_path = self.config.json_annotated_path\n",
    "#         logger.info(f\"Loading annotations from {file_path}.\")\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             annotated_data = json.load(file)\n",
    "        \n",
    "#         # Process annotations to create spaCy Doc objects and save them in a DocBin\n",
    "#         logger.info(\"Processing annotations.\")\n",
    "#         nlp = spacy.blank(\"en\")  # Initialize a blank spaCy model for processing\n",
    "#         doc_bin = self.process_annotations(annotated_data, nlp)\n",
    "        \n",
    "#         # Save processed annotations in .spacy format for later use\n",
    "#         output_path = self.config.output_path\n",
    "#         logger.info(f\"Saving processed annotations to {output_path}.\")\n",
    "#         self.save_spacy_annotations(doc_bin, output_path)\n",
    "        \n",
    "#         # Optional: Read back the saved annotations for verification\n",
    "#         logger.info(\"Reading back saved annotations for verification.\")\n",
    "#         self.read_spacy_annotations(output_path)\n",
    "        \n",
    "#         # Split the processed annotations into training and development data\n",
    "#         logger.info(\"Splitting processed annotations into training and development datasets.\")\n",
    "#         self.split_and_save_data(annotated_data)  # Assumes split_and_save_data handles the split and saving\n",
    "        \n",
    "#         # Train the custom NER model using the split and saved training and development datasets\n",
    "#         logger.info(\"Training the custom NER model.\")\n",
    "#         self.train_ner_model()\n",
    "        \n",
    "#         logger.info(\"SpacyCustomNERModel pipeline completed successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:23,004: 41: career_chief_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:23,013: 41: career_chief_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2024-04-02 17:23:23,019: 41: career_chief_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2024-04-02 17:23:23,027: 64: career_chief_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2024-04-02 17:23:23,028: 22: career_chief_logger: INFO: 3306622033:  Custom NER spaCy Model Training Pipeline initialized successfully.]\n",
      "[2024-04-02 17:23:23,029: 33: career_chief_logger: INFO: 3306622033:  Custom NER spaCy Model Training Pipeline: Fetching model configuration.]\n",
      "[2024-04-02 17:23:23,031: 36: career_chief_logger: INFO: 3306622033:  Custom NER spaCy Model Training Pipeline: Initializing the SpacyCustomNERModel component.]\n",
      "[2024-04-02 17:23:23,295: 39: career_chief_logger: INFO: 3306622033:  Custom NER spaCy Model Training Pipeline: Executing the training pipeline.]\n",
      "[2024-04-02 17:23:23,513: 180: career_chief_logger: INFO: 570671794:  Blank spaCy model initialized.]\n",
      "[2024-04-02 17:23:23,864: 84: career_chief_logger: INFO: 570671794:  Annotations processed successfully.]\n",
      "[2024-04-02 17:23:23,914: 99: career_chief_logger: INFO: 570671794:  Split data into 32 training and 9 development examples.]\n",
      "[2024-04-02 17:23:24,053: 106: career_chief_logger: INFO: 570671794:  Training and development data saved successfully.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:24,348] [INFO] Created vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:24,348: 140: spacy: INFO: initialize:  Created vocabulary]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:24,351] [INFO] Finished initializing nlp object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 17:23:24,351: 151: spacy: INFO: initialize:  Finished initializing nlp object]\n",
      "Losses at iteration 0: {'ner': 19617.19465446472}\n",
      "Losses at iteration 1: {'ner': 10884.20190668106}\n",
      "Losses at iteration 2: {'ner': 6195.7482770085335}\n",
      "Losses at iteration 3: {'ner': 3769.1879676878452}\n",
      "Losses at iteration 4: {'ner': 3778.7230287976563}\n",
      "Losses at iteration 5: {'ner': 3491.7997135845944}\n",
      "Losses at iteration 6: {'ner': 3880.4862605035305}\n",
      "Losses at iteration 7: {'ner': 3045.6036987900734}\n",
      "Losses at iteration 8: {'ner': 3435.81584133883}\n",
      "Losses at iteration 9: {'ner': 2881.633843127638}\n",
      "Losses at iteration 10: {'ner': 3298.4148988546804}\n",
      "Losses at iteration 11: {'ner': 3897.402091026772}\n",
      "Losses at iteration 12: {'ner': 3091.5327505394816}\n",
      "Losses at iteration 13: {'ner': 2934.867629684275}\n",
      "Losses at iteration 14: {'ner': 3521.4135258719325}\n",
      "Losses at iteration 15: {'ner': 2951.705285578966}\n",
      "Losses at iteration 16: {'ner': 2135.759005185566}\n",
      "Losses at iteration 17: {'ner': 2025.7460361494109}\n",
      "Losses at iteration 18: {'ner': 2780.7780348685483}\n",
      "Losses at iteration 19: {'ner': 3029.3448324456112}\n",
      "[2024-04-02 17:25:45,754: 153: career_chief_logger: INFO: 570671794:  Custom NER model trained and saved successfully at artifacts/model_training/spacy_ner/trained_model.]\n",
      "[2024-04-02 17:25:45,790: 198: career_chief_logger: INFO: 570671794:  SpacyCustomNERModel pipeline completed successfully.]\n",
      "[2024-04-02 17:25:45,816: 42: career_chief_logger: INFO: 3306622033:  Custom NER spaCy Model Training Pipeline: Training pipeline executed successfully.]\n"
     ]
    }
   ],
   "source": [
    "from src.career_chief import logger\n",
    "from src.career_chief.config.configuration import ConfigurationManager\n",
    "# from src.career_chief.components.spacy_ner_custom_model import SpacyCustomNERModel\n",
    "\n",
    "class SpacyCustomNERModelPipeline:\n",
    "    \"\"\"\n",
    "    Orchestrates the pipeline for training a custom spaCy Named Entity Recognition (NER) model.\n",
    "\n",
    "    This class manages the end-to-end process of configuring, training, and evaluating\n",
    "    a custom spaCy NER model. It leverages the ConfigurationManager to fetch necessary\n",
    "    configurations and uses the SpacyCustomNERModel component to execute the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Custom NER spaCy Model Training Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with the ConfigurationManager instance\n",
    "        to access the necessary configurations for the NER model training.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "        logger.info(f\"{self.STAGE_NAME} initialized successfully.\")\n",
    "\n",
    "    def run_spacy_custom_ner_model(self):\n",
    "        \"\"\"\n",
    "        Executes the spaCy custom NER model training pipeline.\n",
    "\n",
    "        This method orchestrates the process of loading configurations, processing annotations,\n",
    "        training the model, and optionally evaluating its performance. It handles and logs\n",
    "        any errors that occur during the pipeline execution.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{self.STAGE_NAME}: Fetching model configuration.\")\n",
    "            spacy_custom_ner_model_config = self.config_manager.get_spacy_ner_config()\n",
    "\n",
    "            logger.info(f\"{self.STAGE_NAME}: Initializing the SpacyCustomNERModel component.\")\n",
    "            spacy_model_training = SpacyCustomNERModel(config=spacy_custom_ner_model_config)\n",
    "\n",
    "            logger.info(f\"{self.STAGE_NAME}: Executing the training pipeline.\")\n",
    "            spacy_model_training.run()\n",
    "\n",
    "            logger.info(f\"{self.STAGE_NAME}: Training pipeline executed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{self.STAGE_NAME}: Error occurred during model training - {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = SpacyCustomNERModelPipeline()\n",
    "    pipeline.run_spacy_custom_ner_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "career_chief_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
