{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbookpro/Documents/Documents - Macbookâ€™s MacBook Pro/career/career_chief_rep'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure config.yaml which holds information about where our data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration related to data validation\n",
    "data_validation:\n",
    "  # Directory where data validation results and artifacts are stored\n",
    "  root_dir: artifacts/data_validation\n",
    "  \n",
    "  # Path to the ingested data file that will be used for validation\n",
    "  data_source_file: artifacts/data_ingestion/gsearch_jobs.csv\n",
    "  \n",
    "  # Path to the file that captures the validation status (e.g., success, errors encountered)\n",
    "  status_file: artifacts/data_validation/status.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Entity for Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data validation.\n",
    "    \n",
    "    This class captures the essential configurations required for data validation, \n",
    "    including directories for storing validation results, paths to data files, \n",
    "    and the expected data schema.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    root_dir : Path\n",
    "        Directory for storing validation results and related artifacts.\n",
    "        \n",
    "    data_source_file : Path\n",
    "        Path to the ingested or feature-engineered data file.\n",
    "        \n",
    "    status_file : Path\n",
    "        File for logging the validation status.\n",
    "        \n",
    "    schema : Dict[str, Dict[str, str]]\n",
    "        Dictionary containing initial schema configurations for data validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    root_dir: Path  # Directory for storing validation results and related artifacts\n",
    "    data_source_file: Path  # Path to the ingested or feature-engineered data file\n",
    "    status_file: Path  # File for logging the validation status\n",
    "    schema: Dict[str, Dict[str, str]]  # Dictionary containing initial schema configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "schema_type: \"schemasd\"\n",
    "description: \"Defines the acceptable schema features for the jobs data.\"\n",
    "\n",
    "# Here, we detail the expected structure and data types for each column in the dataset.\n",
    "columns:\n",
    "  date_time:\n",
    "    type: datetime64\n",
    "    description: \"The data and time this job was posted\"\n",
    "  title:\n",
    "    type: string\n",
    "    description: \"The title of the job, indicating the role or position offered.\"\n",
    "  \n",
    "  company_name:\n",
    "    type: string\n",
    "    description: \"The name of the company offering the job.\"\n",
    "  \n",
    "  location: \n",
    "    type: string\n",
    "    description: \"The geographic location or office where the job is based.\"\n",
    "  \n",
    "  via:\n",
    "    type: category\n",
    "    description: \"The source platform or medium through which the job listing was obtained. Utilizing 'category' type for this field optimizes memory usage, as it is efficient for columns with a limited set of unique values.\"\n",
    "  \n",
    "  description:\n",
    "    type: object  # Consider changing to 'string' if this field exclusively contains text.\n",
    "    description: \"A detailed description of the job, including responsibilities, qualifications, and other relevant information. Stored as an 'object' to accommodate mixed data types, but 'string' could be more appropriate for textual data.\"\n",
    "  \n",
    "  job_id: \n",
    "    type: string\n",
    "    description: \"A unique identifier for the job listing, which may include alphanumeric characters.\"\n",
    "  \n",
    "  salary_standardized: \n",
    "    type: float\n",
    "    description: \"The standardized annual salary for the position. Specified as a 'float' to handle numerical values that can represent a wide range of salaries, including those with decimal points.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.career_chief.constants import *\n",
    "from src.career_chief.utils.common import read_yaml, create_directories\n",
    "from src.career_chief import logger\n",
    "from src.career_chief.entity.config_entity import (DataIngestionConfig, DataValidationConfig)\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    ConfigurationManager manages configurations needed for the data pipeline.\n",
    "\n",
    "    The class reads configuration, parameter, and schema settings from specified files\n",
    "    and provides a set of methods to access these settings. It also takes care of\n",
    "    creating necessary directories defined in the configurations.\n",
    "\n",
    "    Attributes:\n",
    "    - config (dict): Configuration settings.\n",
    "    - params (dict): Parameters for the pipeline.\n",
    "    - schema (dict): Schema information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config_filepath = CONFIG_FILE_PATH, \n",
    "                 params_filepath = PARAMS_FILE_PATH, \n",
    "                 schema_filepath = SCHEMA_FILE_PATH) -> None:\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configurations, parameters, and schema.\n",
    "\n",
    "        Args:\n",
    "        - config_filepath (Path): Path to the configuration file.\n",
    "        - params_filepath (Path): Path to the parameters file.\n",
    "        - schema_filepath (Path): Path to the schema file.\n",
    "\n",
    "        Creates:\n",
    "        - Directories specified in the configuration.\n",
    "        \"\"\"\n",
    "        self.config = self._read_config_file(config_filepath, \"config\")\n",
    "        self.params = self._read_config_file(params_filepath, \"params\")\n",
    "        self.schema = self._read_config_file(schema_filepath, \"schema\")\n",
    "\n",
    "        # Create the directory for storing artifacts if it doesn't exist\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def _read_config_file(self, filepath: str, config_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Read a configuration file and return its content.\n",
    "\n",
    "        Args:\n",
    "        - filepath (str): Path to the configuration file.\n",
    "        - config_name (str): Name of the configuration (for logging purposes).\n",
    "\n",
    "        Returns:\n",
    "        - dict: Configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - Exception: If there's an error reading the file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return read_yaml(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {config_name} file: {filepath}. Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Extract and return data ingestion configurations as a DataIngestionConfig object.\n",
    "\n",
    "        This method fetches settings related to data ingestion, like directories and file paths,\n",
    "        and returns them as a DataIngestionConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataIngestionConfig: Object containing data ingestion configuration settings.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_ingestion' attribute does not exist in the config file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_ingestion\n",
    "            # Create the root directory for data ingestion if it doesn't already exist\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            return DataIngestionConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                local_data_file=Path(config.local_data_file),\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            logger.error(\"The 'data_ingestion' attribute does not exist in the config file.\")\n",
    "            raise e\n",
    "        \n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        \"\"\"\n",
    "        Extracts data validation configurations and constructs a DataValidationConfig object.\n",
    "\n",
    "        Returns:\n",
    "        - DataValidationConfig: Object containing data validation configuration.\n",
    "\n",
    "        Raises:\n",
    "        - AttributeError: If the 'data_validation' attribute does not exist in the config.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract data validation configurations\n",
    "            config = self.config.data_validation\n",
    "            \n",
    "            # Extract schema for data validation\n",
    "            \n",
    "            schema = self.schema.columns\n",
    "            logger.info(schema)\n",
    "            \n",
    "            # Ensure the directory for the status file exists\n",
    "            create_directories([os.path.dirname(config.status_file)])\n",
    "\n",
    "            # Construct and return the DataValidationConfig object\n",
    "            return DataValidationConfig(\n",
    "                root_dir=Path(config.root_dir),\n",
    "                data_source_file=Path(config.data_source_file),\n",
    "                status_file=Path(config.status_file),\n",
    "                schema=schema\n",
    "            )\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Log the error and re-raise the exception for handling by the caller\n",
    "            logger.error(\"The 'data_validation' attribute does not exist in the config file.\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.career_chief import logger\n",
    "from src.career_chief.entity.config_entity import DataValidationConfig\n",
    "\n",
    "\n",
    "class DataValidation:\n",
    "    \"\"\"\n",
    "    The DataValidation class ensures the integrity of the dataset by comparing it \n",
    "    against a predefined schema. It verifies the presence and data types of columns \n",
    "    as per the expectations set in the schema.\n",
    "\n",
    "    Attributes:\n",
    "    - df (pd.DataFrame): The data to be validated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define optional and required columns for validation\n",
    "    optional_columns = {'via'}\n",
    "    required_columns = {'title', 'company_name', 'location', 'description', 'job_id', 'salary_standardized'}\n",
    "\n",
    "    def __init__(self, config: DataValidationConfig, file_object=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidation class.\n",
    "        \n",
    "        Depending on the presence of a file_object, it either loads data from the provided \n",
    "        file object or from the specified file in the configuration.\n",
    "\n",
    "        Args:\n",
    "        - config (DataValidationConfig): Configuration settings for data validation.\n",
    "        - file_object (File, optional): A file object containing the dataset.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing DataValidation.\")\n",
    "        self.config = config\n",
    "        try:\n",
    "            if file_object:\n",
    "                self.df = pd.read_csv(file_object)\n",
    "            else:\n",
    "                self.df = pd.read_csv(self.config.data_source_file, dtype={\n",
    "                    'title': 'string',\n",
    "                    'company_name': 'string',\n",
    "                    'location': 'string',\n",
    "                    'via': 'category',\n",
    "                    'description': 'object',\n",
    "                    'job_id': 'string',\n",
    "                    'salary_standardized': 'float'\n",
    "                }, parse_dates=['date_time'])\n",
    "\n",
    "                # List of columns to keep (based on your schema)\n",
    "                schema_columns = ['title', 'company_name', 'location', 'via', 'description', 'job_id', 'salary_standardized']\n",
    "                \n",
    "                # Drop columns not in the schema\n",
    "                self.df = self.df[schema_columns]\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_source_file}\")\n",
    "            raise\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def validate_all_features(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if all expected columns, as defined in the schema, are present in the dataframe.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all columns are present and match the schema, False otherwise.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting feature validation.\")\n",
    "        \n",
    "        validation_status = True\n",
    "\n",
    "        all_columns = set(self.df.columns)\n",
    "        expected_columns = set(self.config.schema.keys())\n",
    "\n",
    "        missing_required_columns = self.required_columns - all_columns\n",
    "        extra_columns = all_columns - expected_columns - self.optional_columns\n",
    "\n",
    "        if missing_required_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Missing required columns: {', '.join(missing_required_columns)}\")\n",
    "\n",
    "        if extra_columns:\n",
    "            validation_status = False\n",
    "            logger.warning(f\"Extra columns found: {', '.join(extra_columns)}\")\n",
    "\n",
    "        if validation_status:\n",
    "            logger.info(\"All expected columns are present in the dataframe.\")\n",
    "        return validation_status\n",
    "\n",
    "    def validate_data_types(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks the data types of each column in the dataframe against the expected \n",
    "        data types defined in the schema.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if all column data types match the schema, False otherwise.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data type validation.\")\n",
    "        validation_status = True\n",
    "        \n",
    "        expected_data_types = {col: self.config.schema[col]['type'] for col in self.config.schema if col in self.df.columns}\n",
    "\n",
    "        for column, dtype in expected_data_types.items():\n",
    "            if not pd.api.types.is_dtype_equal(self.df[column].dtype, dtype):\n",
    "                validation_status = False\n",
    "                logger.warning(f\"Data type mismatch for column '{column}': Expected {dtype} but got {self.df[column].dtype}\")\n",
    "\n",
    "        if validation_status:\n",
    "            logger.info(\"All data types are as expected.\")\n",
    "        return validation_status\n",
    "\n",
    "\n",
    "\n",
    "    def _write_status_to_file(self, message: str, overwrite: bool = False):\n",
    "        \"\"\"\n",
    "        Writes the validation status message to a specified file.\n",
    "\n",
    "        Args:\n",
    "        - message (str): The message to write.\n",
    "        - overwrite (bool, optional): If set to True, overwrites the file. If False, appends to the file.\n",
    "        \"\"\"\n",
    "        logger.info(\"Writing validation status to file.\")\n",
    "        mode = 'w' if overwrite else 'a'\n",
    "        try:\n",
    "            with open(self.config.status_file, mode) as f:\n",
    "                f.write(message + \"\\n\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to status file: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_all_validations(self) -> bool:\n",
    "        \"\"\"\n",
    "        Executes all data validations and logs the overall status. \n",
    "        It encompasses both feature existence and data type checks.\n",
    "        \"\"\"\n",
    "        logger.info(\"Running all data validations.\")\n",
    "        feature_validation_status = self.validate_all_features()\n",
    "        data_type_validation_status = self.validate_data_types()\n",
    "\n",
    "        overall_status = \"Overall Validation Status: \"\n",
    "        if feature_validation_status and data_type_validation_status:\n",
    "            overall_status += \"All validations passed.\"\n",
    "            logger.info(overall_status)\n",
    "        else:\n",
    "            overall_status += \"Some validations failed. Check the log for details.\"\n",
    "            logger.error(overall_status)\n",
    "        \n",
    "        self._write_status_to_file(overall_status)\n",
    "        return feature_validation_status and data_type_validation_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-04 19:45:58,170: 41: career_chief_logger: INFO: common:  yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-04 19:45:58,172: 41: career_chief_logger: INFO: common:  yaml file: params.yaml loaded successfully]\n",
      "[2024-03-04 19:45:58,175: 41: career_chief_logger: INFO: common:  yaml file: schema.yaml loaded successfully]\n",
      "[2024-03-04 19:45:58,176: 64: career_chief_logger: INFO: common:  Created directory at: artifacts]\n",
      "[2024-03-04 19:45:58,177: 52: career_chief_logger: INFO: 1355205439:  >>>>>> Stage: Data Validation Pipeline started <<<<<<]\n",
      "[2024-03-04 19:45:58,179: 30: career_chief_logger: INFO: 1355205439:  Fetching initial data validation configuration...]\n",
      "[2024-03-04 19:45:58,180: 108: career_chief_logger: INFO: 1156641677:  {'date_time': {'type': 'datetime64', 'description': 'The data and time this job was posted'}, 'title': {'type': 'string', 'description': 'The title of the job, indicating the role or position offered.'}, 'company_name': {'type': 'string', 'description': 'The name of the company offering the job.'}, 'location': {'type': 'string', 'description': 'The geographic location or office where the job is based.'}, 'via': {'type': 'category', 'description': \"The source platform or medium through which the job listing was obtained. Utilizing 'category' type for this field optimizes memory usage, as it is efficient for columns with a limited set of unique values.\"}, 'description': {'type': 'object', 'description': \"A detailed description of the job, including responsibilities, qualifications, and other relevant information. Stored as an 'object' to accommodate mixed data types, but 'string' could be more appropriate for textual data.\"}, 'job_id': {'type': 'string', 'description': 'A unique identifier for the job listing, which may include alphanumeric characters.'}, 'salary_standardized': {'type': 'float', 'description': \"The standardized annual salary for the position. Specified as a 'float' to handle numerical values that can represent a wide range of salaries, including those with decimal points.\"}}]\n",
      "[2024-03-04 19:45:58,181: 64: career_chief_logger: INFO: common:  Created directory at: artifacts/data_validation]\n",
      "[2024-03-04 19:45:58,184: 33: career_chief_logger: INFO: 1355205439:  Initializing data validation process...]\n",
      "[2024-03-04 19:45:58,185: 31: career_chief_logger: INFO: 85106482:  Initializing DataValidation.]\n",
      "[2024-03-04 19:46:00,127: 36: career_chief_logger: INFO: 1355205439:  Executing Data Validations...]\n",
      "[2024-03-04 19:46:00,128: 136: career_chief_logger: INFO: 85106482:  Running all data validations.]\n",
      "[2024-03-04 19:46:00,129: 68: career_chief_logger: INFO: 85106482:  Starting feature validation.]\n",
      "[2024-03-04 19:46:00,129: 87: career_chief_logger: INFO: 85106482:  All expected columns are present in the dataframe.]\n",
      "[2024-03-04 19:46:00,129: 98: career_chief_logger: INFO: 85106482:  Starting data type validation.]\n",
      "[2024-03-04 19:46:00,130: 109: career_chief_logger: INFO: 85106482:  All data types are as expected.]\n",
      "[2024-03-04 19:46:00,130: 143: career_chief_logger: INFO: 85106482:  Overall Validation Status: All validations passed.]\n",
      "[2024-03-04 19:46:00,130: 122: career_chief_logger: INFO: 85106482:  Writing validation status to file.]\n",
      "[2024-03-04 19:46:00,131: 39: career_chief_logger: INFO: 1355205439:  Data Validation Pipeline completed successfully.]\n",
      "[2024-03-04 19:46:00,144: 54: career_chief_logger: INFO: 1355205439:  >>>>>> Stage Data Validation Pipeline completed <<<<<< \n",
      "\n",
      "x==========x]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.career_chief import logger\n",
    "\n",
    "class DataValidationPipeline:\n",
    "    \"\"\"\n",
    "    This pipeline handles the initial data validation steps.\n",
    "    After the data ingestion stage, it's imperative to ensure the data's integrity\n",
    "    before moving on to feature engineering or model training. This class\n",
    "    orchestrates that validation by checking for correct features and data types.\n",
    "\n",
    "    Attributes:\n",
    "        STAGE_NAME (str): The name of this pipeline stage.\n",
    "    \"\"\"\n",
    "\n",
    "    STAGE_NAME = \"Data Validation Pipeline\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with a configuration manager.\n",
    "        \"\"\"\n",
    "        self.config_manager = ConfigurationManager()\n",
    "\n",
    "    def run_data_validation(self):\n",
    "        \"\"\"\n",
    "        Run the set of data validations.\n",
    "        \n",
    "        This method orchestrates the different validation functions to ensure the\n",
    "        dataset's integrity.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Fetching initial data validation configuration...\")\n",
    "            data_validation_config = self.config_manager.get_data_validation_config()\n",
    "\n",
    "            logger.info(\"Initializing data validation process...\")\n",
    "            data_validation = DataValidation(config=data_validation_config)\n",
    "\n",
    "            logger.info(\"Executing Data Validations...\")\n",
    "            data_validation.run_all_validations()\n",
    "\n",
    "            logger.info(\"Data Validation Pipeline completed successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the data validation: {e}\")\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the entire Initial Data Validation Pipeline.\n",
    "        \n",
    "        This method encapsulates the process of the initial data validation and\n",
    "        provides logs for each stage of the pipeline.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\">>>>>> Stage: {DataValidationPipeline.STAGE_NAME} started <<<<<<\")\n",
    "            self.run_data_validation()\n",
    "            logger.info(f\">>>>>> Stage {DataValidationPipeline.STAGE_NAME} completed <<<<<< \\n\\nx==========x\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error encountered during the {DataValidationPipeline.STAGE_NAME}: {e}\")\n",
    "            raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pipeline = DataValidationPipeline()\n",
    "    pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "career_chief_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
